import os
import time
import re
import json
import psycopg2
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# --- í™˜ê²½ ì„¤ì • ---
def load_env(path: Path) -> None:
    if not path.exists(): return
    for line in path.read_text(encoding="utf-8").splitlines():
        if not line or line.startswith("#") or "=" not in line: continue
        key, value = line.split("=", 1)
        os.environ.setdefault(key.strip(), value.strip())

load_env(Path(__file__).with_name(".env"))

DB_CONFIG = {
    "host": os.getenv("DB_HOST", "localhost"),
    "database": os.getenv("DB_NAME", "sportslab"),
    "user": os.getenv("DB_USER", "postgres"),
    "password": os.getenv("DB_PASSWORD", "rootpassword"),
    "port": os.getenv("DB_PORT", "5432"),
}

class KBLFullScraper:
    def __init__(self):
        options = webdriver.ChromeOptions()
        # [ì¤‘ìš”] í™”ë©´ì„ ë„ì›Œì•¼ ì°¨ë‹¨ë˜ì§€ ì•ŠìŒ (Headless ì£¼ì„ ì²˜ë¦¬)
        # options.add_argument('--headless=new')
        
        options.add_argument('--start-maximized') 
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-blink-features=AutomationControlled') 
        options.add_experimental_option("excludeSwitches", ["enable-automation"]) 
        options.add_experimental_option('useAutomationExtension', False)
        
        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        
        # ë´‡ íƒì§€ íšŒí”¼ìš© JS
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        self.conn = psycopg2.connect(**DB_CONFIG)
        self.cur = self.conn.cursor()

    def __del__(self):
        if hasattr(self, 'driver'): self.driver.quit()
        if hasattr(self, 'cur'): self.cur.close()
        if hasattr(self, 'conn'): self.conn.close()

    # =========================================================================
    # 1. íŒ€ ì†Œê°œ ìˆ˜ì§‘
    # URL: https://www.kbl.or.kr/team/intro
    # =========================================================================
    def scrape_teams(self):
        print("\nğŸ€ [1ë‹¨ê³„] íŒ€ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...")
        self.driver.get("https://www.kbl.or.kr/team/intro")
        time.sleep(3) # ë¡œë”© ëŒ€ê¸°
        
        try:
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.CLASS_NAME, "team_list"))
            )
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            team_list = soup.select(".team_list li")
            
            print(f"  ğŸ‘‰ ì´ {len(team_list)}ê°œ êµ¬ë‹¨ ë°œê²¬")
            
            for team in team_list:
                name = team.select_one(".name").get_text(strip=True)
                # ë§í¬ì—ì„œ êµ¬ë‹¨ ì½”ë“œ/ID ì¶”ì¶œ
                link = team.select_one("a")['href'] # ì˜ˆ: /team/intro/10
                team_kbl_id = link.split("/")[-1]
                
                # DB ì €ì¥ (sl_teams í…Œì´ë¸”ì´ ìˆë‹¤ê³  ê°€ì •)
                self.save_team(team_kbl_id, name)
                
        except Exception as e:
            print(f"  âŒ íŒ€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    def save_team(self, kbl_id, name):
        try:
            # Upsert (ì´ë¯¸ ìˆìœ¼ë©´ ì´ë¦„ë§Œ ì—…ë°ì´íŠ¸)
            sql = """
                INSERT INTO sl_teams (name, created_at, updated_at)
                VALUES (%s, NOW(), NOW())
                ON CONFLICT (name) DO UPDATE SET updated_at = NOW()
                RETURNING id;
            """
            # ì°¸ê³ : ì‹¤ì œë¡œëŠ” KBL IDë¥¼ ë§¤í•‘í•˜ëŠ” ë¡œì§ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
            self.cur.execute(sql, (name,))
            self.conn.commit()
            print(f"    ğŸ’¾ ì €ì¥: {name} (KBL_ID: {kbl_id})")
        except Exception:
            self.conn.rollback()

    # =========================================================================
    # 2. ì„ ìˆ˜ ëª©ë¡ ìˆ˜ì§‘ (í˜ì´ì§• í¬í•¨)
    # URL: https://www.kbl.or.kr/player/player
    # =========================================================================
    def scrape_players(self):
        print("\nğŸ€ [2ë‹¨ê³„] ì„ ìˆ˜ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘...")
        self.driver.get("https://www.kbl.or.kr/player/player")
        time.sleep(4)
        
        try:
            # í…Œì´ë¸” ë¡œë”© í™•ì¸
            WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, "player_list")))
            
            # ì „ì²´ í˜ì´ì§€ ìˆ˜ íŒŒì•… (ë°ìŠ¤í¬íƒ‘ ë·° ê¸°ì¤€)
            paging_box = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.page.desktop select"))
            )
            total_pages = len(paging_box.find_elements(By.TAG_NAME, "option"))
            print(f"  ğŸ‘‰ ì´ {total_pages} í˜ì´ì§€ ê°ì§€ë¨")

            for page_num in range(1, total_pages + 1):
                print(f"  ğŸ”„ {page_num}/{total_pages} í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")
                
                try:
                    # í˜ì´ì§€ ì´ë™ ë¡œì§
                    select_elem = WebDriverWait(self.driver, 10).until(
                        EC.element_to_be_clickable((By.CSS_SELECTOR, "div.page.desktop select"))
                    )
                    select = Select(select_elem)
                    
                    if select.first_selected_option.get_attribute("value") != str(page_num):
                        select.select_by_value(str(page_num))
                        time.sleep(2) # ë°ì´í„° ë¡œë”© ëŒ€ê¸°
                        WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, "player_list")))
                    
                    # íŒŒì‹±
                    self.parse_and_save_players()
                    
                except Exception as e:
                    print(f"    âš ï¸ {page_num}í˜ì´ì§€ ì—ëŸ¬: {e}")
                    self.driver.refresh()
                    time.sleep(3)
                    continue

        except Exception as e:
            print(f"  âŒ ì„ ìˆ˜ ìˆ˜ì§‘ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def parse_and_save_players(self):
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        rows = soup.select(".player_list tbody tr")
        
        count = 0
        for row in rows:
            try:
                cols = row.find_all("td")
                if len(cols) < 5: continue
                
                # ë°ì´í„° ì¶”ì¶œ
                name_tag = cols[1].select_one(".player_name a")
                if not name_tag: continue
                
                name = name_tag.get_text(strip=True)
                player_id = name_tag['href'].split("/")[-1]
                
                position = cols[2].get_text(strip=True)
                height = int(re.sub(r'[^\d]', '', cols[3].get_text(strip=True) or "0"))
                team_name = cols[4].get_text(strip=True)
                
                # DB ì €ì¥
                self.save_player(player_id, name, team_name, position, height)
                count += 1
            except: continue
        print(f"    âœ… {count}ëª… ì €ì¥ ì™„ë£Œ")

    def save_player(self, pid, name, team_name, position, height):
        try:
            # íŒ€ ID ì¡°íšŒ
            self.cur.execute("SELECT id FROM sl_teams WHERE name LIKE %s LIMIT 1", (f"%{team_name}%",))
            res = self.cur.fetchone()
            # íŒ€ì´ DBì— ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê±°ë‚˜ ìƒì„± (ì—¬ê¸°ì„  ê±´ë„ˆëœ€)
            if not res: return 
            
            biometrics = {"height_cm": height, "position": position, "kbl_id": pid}
            
            sql = """
                INSERT INTO sl_players (id, name, biometrics, created_at, updated_at)
                VALUES (%s, %s, %s, NOW(), NOW())
                ON CONFLICT (id) DO UPDATE 
                SET name = EXCLUDED.name, biometrics = sl_players.biometrics || EXCLUDED.biometrics, updated_at = NOW();
            """
            self.cur.execute(sql, (pid, name, json.dumps(biometrics)))
            self.conn.commit()
        except Exception:
            self.conn.rollback()

    # =========================================================================
    # 3. ê²½ê¸° ê²°ê³¼/ì¼ì • ìˆ˜ì§‘
    # URL: https://www.kbl.or.kr/match/schedule?type=SCHEDULE
    # =========================================================================
    def scrape_schedule(self):
        print("\nğŸ€ [3ë‹¨ê³„] ê²½ê¸° ì¼ì • ìˆ˜ì§‘ ì‹œì‘...")
        self.driver.get("https://www.kbl.or.kr/match/schedule?type=SCHEDULE")
        time.sleep(5)
        
        try:
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.CLASS_NAME, "schedule_list"))
            )
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            days = soup.select(".schedule_list .day_list")
            print(f"  ğŸ‘‰ ìº˜ë¦°ë” ë¡œë”© ì™„ë£Œ ({len(days)}ì¼ì¹˜ ë°ì´í„°)")
            
            for day in days:
                date_str = day.select_one(".date").get_text(strip=True) # ì˜ˆ: 10.19 (í† )
                matches = day.select("li")
                
                for match in matches:
                    home = match.select_one(".team.home .name").get_text(strip=True)
                    score_home = match.select_one(".team.home .score").get_text(strip=True)
                    
                    away = match.select_one(".team.away .name").get_text(strip=True)
                    score_away = match.select_one(".team.away .score").get_text(strip=True)
                    
                    state = match.select_one(".state").get_text(strip=True) # ì¢…ë£Œ, ì˜ˆì •
                    
                    print(f"    ğŸ“… [{date_str}] {home} {score_home} : {score_away} {away} ({state})")
                    # TODO: sl_matches í…Œì´ë¸”ì— INSERT ë¡œì§ ì¶”ê°€
                    # self.save_match(...) 

        except Exception as e:
            print(f"  âŒ ì¼ì • ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    def run(self):
        self.scrape_teams()
        self.scrape_players()
        self.scrape_schedule()

if __name__ == "__main__":
    scraper = KBLFullScraper()
    scraper.run()